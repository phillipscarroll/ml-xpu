{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7mWFNse-pzE"
      },
      "source": [
        "# Train And Test - Classification XPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLiQBnz-pzH"
      },
      "source": [
        "## Classification\n",
        "\n",
        "- Imports\n",
        "  - standard libs\n",
        "  - 3rd party libs\n",
        "  - alpabetical or logical grouping\n",
        "- Set random seed\n",
        "- Config and Hyperparams\n",
        "- Dataset and Dataloader\n",
        "- Model definition/class\n",
        "- Helper functions (training, eval, visualization)\n",
        "- Then main code\n",
        "\n",
        "Note: You can flip torch.amp on and off to test, this is work on XPU. Note this is not a great example case for leveraging amp but it is functional for testing. This is a setting with the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Static seed for reproducibility or turn on randomization\n",
        "RANDOM_SEED = 42\n",
        "RANDOMIZE_SEED = False\n",
        "\n",
        "# Check if we should use a static seed, if not randomize it\n",
        "if RANDOMIZE_SEED:\n",
        "    RANDOM_SEED = random.randint(0, 1000000000)\n",
        "    print(f\"Using seed: {RANDOM_SEED}\")\n",
        "\n",
        "# Setup device agnostic code, you can extend to include cuda as well\n",
        "# device = \"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.xpu.is_available else \"cpu\"\n",
        "# This will try to use xpu, fallback to cuda, then to cpu, I have this in my mixed gpu environments\n",
        "device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize Hyperparameters\n",
        "# Make n samples for dataset\n",
        "n_samples = 5000\n",
        "test_size = 0.2 # 0.2 = 20% test, 80% train\n",
        "learning_rate = 0.02\n",
        "noise = 0.03 # Noise for circles which will make them not perfect circles\n",
        "epochs = 5000\n",
        "input_features = 2\n",
        "output_features = 1\n",
        "hidden_units = 24\n",
        "# Set to True to use mixed precision training (automatic mixed precision)\n",
        "# This will be less accurate when turned on but technically faster\n",
        "use_amp = True\n",
        "\n",
        "# Generate the dataset\n",
        "# Create Circles\n",
        "X, y = make_circles(n_samples, noise = noise, random_state = RANDOM_SEED)\n",
        "\n",
        "plt.scatter(x = X[:, 0],\n",
        "            y = X[:, 1],\n",
        "            c = y,\n",
        "            cmap = plt.cm.RdYlBu);\n",
        "\n",
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Scikit learn has methods to do Train/Test split on data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = test_size, \n",
        "                                                    random_state = RANDOM_SEED)\n",
        "\n",
        "# Build model - multi-class\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, input_features = 2, output_features = 1, hidden_units = 8): # defaults\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features = input_features, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = output_features),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer_stack(x)\n",
        "\n",
        "# Calculate accuracy out of 100 examples\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # item gets the value out as a single item, this also moves it to cpu\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "# Set with hyperparameters    \n",
        "model_0 = ClassificationModel(input_features = input_features, output_features = output_features, hidden_units = hidden_units).to(device)\n",
        "\n",
        "# Make predictions\n",
        "with torch.inference_mode():\n",
        "    untrained_preds = model_0(X_test.to(device)) # pass test data to model/device\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() # Sigmoid activation function built in, this is the numerical stable way\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(params = model_0.parameters(), lr = learning_rate)\n",
        "\n",
        "# View first 5 outputs of the forweard pass on the test data\n",
        "model_0.eval() # Use training mode when making predictions\n",
        "with torch.inference_mode(): # Use inference mode when making predictions\n",
        "    y_logits = model_0(X_test.to(device))[:5]\n",
        "\n",
        "y_pred_probs = torch.sigmoid(y_logits)\n",
        "\n",
        "##### Find the predicted labels\n",
        "# We got raw logits, then turned them into pred probs, now we need pred labels\n",
        "y_preds = torch.round(y_pred_probs) # predicted labels\n",
        "\n",
        "# In Full (logits -> pred probs -> pred labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.xpu.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Put data on target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Initialize GradScaler for mixed precision training\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "# Building and eval loop\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model_0.train()\n",
        "\n",
        "    if use_amp:\n",
        "        # Forward pass with autocast for mixed precision\n",
        "        with torch.amp.autocast(device):\n",
        "            # Forward Pass\n",
        "            y_logits = model_0(X_train).squeeze() # pass raw logits\n",
        "            y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "    else:\n",
        "        # Forward Pass\n",
        "        y_logits = model_0(X_train).squeeze() # pass raw logits\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "\n",
        "    loss = loss_fn(y_logits, y_train)\n",
        "    acc = accuracy_fn(y_true = y_train, y_pred = y_pred)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass with GradScaler for mixed precision\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        if use_amp:\n",
        "            # Forward pass with autocast for mixed precision\n",
        "            with torch.amp.autocast(device):\n",
        "                # Testing forward pass\n",
        "                test_logits = model_0(X_test).squeeze()\n",
        "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "        else:\n",
        "            # Testing forward pass\n",
        "            test_logits = model_0(X_test).squeeze()\n",
        "            test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "            \n",
        "        # Testing Calc loss and accuracy\n",
        "        test_loss = loss_fn(test_logits, y_test)\n",
        "        test_acc = accuracy_fn(y_true = y_test, y_pred = test_pred)\n",
        "\n",
        "    # Print output\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}% | Device: {device}\")\n",
        "\n",
        "# Download helper func from learn pytorch repo if its not downlaoded\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "    print(\"File exists, skipping download\")\n",
        "else:\n",
        "    print(\"Downloading\")\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        f.write(request.content)\n",
        "\n",
        "# Then import the file\n",
        "from helper_functions import plot_predictions, plot_decision_boundary\n",
        "\n",
        "# Plot decision boundary of the model\n",
        "plt.figure(figsize = (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
