{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7mWFNse-pzE"
      },
      "source": [
        "# Train And Test - Classification XPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLiQBnz-pzH"
      },
      "source": [
        "## Classification\n",
        "\n",
        "- Imports\n",
        "  - standard libs\n",
        "  - 3rd party libs\n",
        "  - alpabetical or logical grouping\n",
        "- Set random seed\n",
        "- Config and Hyperparams\n",
        "- Dataset and Dataloader\n",
        "- Model definition/class\n",
        "- Helper functions (training, eval, visualization)\n",
        "- Then main code\n",
        "\n",
        "Note: You can flip torch.amp on and off to test, this is work on XPU. Note this is not a great example case for leveraging amp but it is functional for testing. This is a setting with the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set or adjust your hyperparameters and amp/cpu override\n",
        "hp = {\n",
        "    \"n_samples\": 5000,\n",
        "    \"test_size\": 0.2,\n",
        "    \"learning_rate\": 0.025,\n",
        "    \"noise\": 0.03,\n",
        "    \"epochs\": 3000,\n",
        "    \"input_features\": 2,\n",
        "    \"output_features\": 1,\n",
        "    \"hidden_units\": 24,\n",
        "    \"random_seed\": 42,\n",
        "    \"randomize_seed\": True,\n",
        "    \"cpu_only\": False,\n",
        "}\n",
        "# Logging configuration\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Randomize seed if set to True\n",
        "if hp['randomize_seed']:\n",
        "    hp['random_seed'] = random.randint(0, 1000000000)\n",
        "logging.info(f\"Seed set to: {hp['random_seed']}\")   \n",
        "\n",
        "def get_device():\n",
        "    \"\"\"\n",
        "    This will check for an Intel XPU device and return it if available, otherwise it will return cpu.\n",
        "\n",
        "    Returns the torch device to use.\n",
        "    \"\"\"\n",
        "    if hp['cpu_only'] == False:\n",
        "        #device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
        "        if torch.xpu.is_available():\n",
        "            device = \"xpu\"\n",
        "        elif torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "\n",
        "        logging.info(f\"Using device: {device}\")\n",
        "        return device\n",
        "    else:\n",
        "        logging.info(\"Using CPU only\")\n",
        "        return \"cpu\"\n",
        "\n",
        "# Basic Classification Model with ReLU activations\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, input_features = 2, output_features = 1, hidden_units = 8):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features = input_features, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = hidden_units, out_features = output_features),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.linear_layer_stack(x)\n",
        "\n",
        "def generate_data(samples, noise, seed, size):\n",
        "    \"\"\"\n",
        "    This function generates a dataset using sklearn's make_circles function.\n",
        "\n",
        "    samples: int, number of samples to generate\n",
        "    noise: float, standard deviation of Gaussian noise added to the data\n",
        "    seed: int, random seed for reproducibility\n",
        "    size: float, size of the test set\n",
        "\n",
        "    Returns the train and test sets as tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the dataset\n",
        "    X, y = make_circles(samples, noise=noise, random_state=seed)\n",
        "\n",
        "    # Plot the data\n",
        "    plt.scatter(x = X[:, 0], y = X[:, 1], c = y, cmap = plt.cm.RdYlBu)\n",
        "\n",
        "    # Turn data into tensors\n",
        "    X = torch.from_numpy(X).type(torch.float)\n",
        "    y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, random_state=seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Calculate accuracy out of 100 examples\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "def import_helper_func():\n",
        "    if Path(\"helper_functions.py\").is_file():\n",
        "        print(\"File exists, skipping download\")\n",
        "    else:\n",
        "        print(\"Downloading\")\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "        with open(\"helper_functions.py\", \"wb\") as f:\n",
        "            f.write(request.content)\n",
        "\n",
        "    from helper_functions import plot_predictions, plot_decision_boundary\n",
        "\n",
        "def set_seed(seed=hp['random_seed'], device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    if device == \"xpu\":\n",
        "        torch.xpu.manual_seed(seed)\n",
        "    elif device == \"cuda\":\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    logging.info(f\"Seed has been set: {seed}\")\n",
        "\n",
        "def move_to_device(X_train, y_train, X_test, y_test, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Moves data to the target device.\n",
        "    \"\"\"\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    logging.info(f\"X_train device: {X_train.device}, y_train device: {y_train.device}\")\n",
        "    logging.info(f\"X_test device: {X_test.device}, y_test device: {y_test.device}\")\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def train_test_model(model, X_train, y_train, X_test, y_test, loss_fn, optimizer, epochs, device=\"cpu\", scaler=None):\n",
        "    \"\"\"\n",
        "    Train and test a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model.\n",
        "        X_train (torch.Tensor): Training data (features).\n",
        "        y_train (torch.Tensor): Training labels.\n",
        "        X_test (torch.Tensor): Testing data (features).\n",
        "        y_test (torch.Tensor): Testing labels.\n",
        "        loss_fn (torch.nn.Module): An instance of a PyTorch loss function.\n",
        "        optimizer (torch.optim.Optimizer): An instance of a PyTorch optimizer.\n",
        "        epochs (int): Number of epochs to train the model for.\n",
        "        device (str): The target device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Training loss, testing loss\n",
        "    \"\"\"\n",
        "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
        "        model.train()\n",
        "\n",
        "        if device != \"cpu\":\n",
        "            with torch.amp.autocast(device):\n",
        "\n",
        "                y_logits = model(X_train).squeeze()\n",
        "                y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "                loss = loss_fn(y_logits, y_train)\n",
        "                acc = accuracy_fn(y_true = y_train, y_pred = y_pred)\n",
        "                optimizer.zero_grad()\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "        else:\n",
        "            y_logits = model(X_train).squeeze()\n",
        "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "            loss = loss_fn(y_logits, y_train)\n",
        "            acc = accuracy_fn(y_true = y_train, y_pred = y_pred)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "\n",
        "            if device != \"cpu\":\n",
        "                with torch.amp.autocast(device):\n",
        "                    test_logits = model(X_test).squeeze()\n",
        "                    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "            else:\n",
        "                test_logits = model(X_test).squeeze()\n",
        "                test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "                \n",
        "            test_loss = loss_fn(test_logits, y_test)\n",
        "            test_acc = accuracy_fn(y_true = y_test, y_pred = test_pred)\n",
        "\n",
        "    logging.info(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}% | Device: {device}\")\n",
        "\n",
        "# Download helper func from learn pytorch repo if its not downlaoded\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "    print(\"File exists, skipping download\")\n",
        "else:\n",
        "    print(\"Downloading\")\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        f.write(request.content)\n",
        "\n",
        "# Then import the file\n",
        "from helper_functions import plot_predictions, plot_decision_boundary\n",
        "\n",
        "# Main function to run training, testing, saving of the model\n",
        "def main():\n",
        "\n",
        "    # Set the device\n",
        "    device = get_device()\n",
        "\n",
        "    # Download helper functions\n",
        "    import_helper_func()\n",
        "\n",
        "    # Call the function to generate the data\n",
        "    X_train, X_test, y_train, y_test = generate_data(samples=hp['n_samples'], noise=hp['noise'], seed=hp['random_seed'], size=hp['test_size'])\n",
        "\n",
        "    # Set with hyperparameters    \n",
        "    model_0 = ClassificationModel(input_features=hp['input_features'], output_features=hp['output_features'], hidden_units=hp['hidden_units']).to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.inference_mode():\n",
        "        untrained_preds = model_0(X_test.to(device)) # pass test data to model/device\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(params=model_0.parameters(), lr=hp['learning_rate'])\n",
        "\n",
        "    # View first 5 outputs of the forweard pass on the test data\n",
        "    model_0.eval() # Use training mode when making predictions\n",
        "    with torch.inference_mode(): # Use inference mode when making predictions\n",
        "        y_logits = model_0(X_test.to(device))[:5]\n",
        "\n",
        "    y_pred_probs = torch.sigmoid(y_logits)\n",
        "\n",
        "    ##### Find the predicted labels\n",
        "    # We got raw logits, then turned them into pred probs, now we need pred labels\n",
        "    y_preds = torch.round(y_pred_probs) # predicted labels\n",
        "\n",
        "    # In Full (logits -> pred probs -> pred labels)\n",
        "    y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "\n",
        "    # Get rid of extra dimension\n",
        "    y_preds.squeeze()\n",
        "\n",
        "    set_seed(hp['random_seed'], device)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = move_to_device(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, device=device)\n",
        "\n",
        "    if hp['cpu_only'] == False:\n",
        "        scaler = torch.amp.GradScaler(device)\n",
        "        train_test_model(model_0, X_train, y_train, X_test, y_test, loss_fn, optimizer, hp['epochs'], device, scaler)\n",
        "    else:\n",
        "        train_test_model(model_0, X_train, y_train, X_test, y_test, loss_fn, optimizer, hp['epochs'], device)\n",
        "\n",
        "    # Plot decision boundary of the model\n",
        "    plt.figure(figsize = (12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Train\")\n",
        "    plot_decision_boundary(model_0, X_train, y_train)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Test\")\n",
        "    plot_decision_boundary(model_0, X_test, y_test)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
