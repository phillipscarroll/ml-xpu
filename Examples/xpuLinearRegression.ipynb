{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7mWFNse-pzE"
      },
      "source": [
        "# Train And Test - Regression XPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLiQBnz-pzH"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "- Imports\n",
        "  - standard libs\n",
        "  - 3rd party libs\n",
        "  - alpabetical or logical grouping\n",
        "- Set random seed\n",
        "- Config and Hyperparams\n",
        "- Dataset and Dataloader\n",
        "- Model definition/class\n",
        "- Helper functions (training, eval, visualization)\n",
        "- Then main code\n",
        "\n",
        "Note: You can flip torch.amp on and off to test, this is work on XPU. Note this is not a great example case for leveraging amp but it is functional for testing. This is a setting with the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "# Random seed number\n",
        "RANDOM_SEED = 1\n",
        "\n",
        "# Setup device agnostic code, you can extend to include cuda as well\n",
        "# device = \"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "# This will try to use xpu, fallback to cuda, then to cpu, I have this in my mixed gpu environments\n",
        "device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize Hyperparameters\n",
        "# Use step to increase or decrease the dataset size, lower number = more data, higher number = less data\n",
        "start, end, step, weight, bias = 0, 1, 0.0002, 0.7, 0.3\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "# Set to True to use mixed precision training (automatic mixed precision)\n",
        "# This will be less accurate when turned on but technically faster\n",
        "use_amp = True\n",
        "\n",
        "# Subclass nn.Module to make our model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use nn.Linear() for creating the model parameters\n",
        "        self.linear_layer = nn.Linear(in_features=1, \n",
        "                                      out_features=1)\n",
        "    \n",
        "    # Define the forward computation (input data x flows through nn.Linear())\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "# Generate dataset and return the training and test data\n",
        "def generate_dataset(start=0, end=1, step=0.0002, weight=0.7, bias=0.3):\n",
        "    \"\"\"\n",
        "    Generate a dataset for a sample linear regression problem.\n",
        "\n",
        "    Args:\n",
        "        start (float): The start of the X values.\n",
        "        end (float): The end of the X values.\n",
        "        step (float): The step between X values.\n",
        "        weight (float): The weight of the linear equation.\n",
        "        bias (float): The bias of the linear equation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: X_train, y_train, X_test, y_test\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create X and y (features and labels)\n",
        "    X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\n",
        "    y = weight * X + bias \n",
        "    X[:10], y[:10]\n",
        "    \n",
        "    # Split data\n",
        "    train_split = int(0.8 * len(X))\n",
        "    X_train, y_train = X[:train_split], y[:train_split]\n",
        "    X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "    # Return data\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Generate dataset and return the training and test data\n",
        "X_train, y_train, X_test, y_test = generate_dataset(start, end, step, weight, bias)\n",
        "\n",
        "# Plotting function to visualize the data\n",
        "def plot_predictions(train_data=X_train.cpu(), \n",
        "                     train_labels=y_train.cpu(), \n",
        "                     test_data=X_test.cpu(), \n",
        "                     test_labels=y_test.cpu(), \n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "  \n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});\n",
        "\n",
        "\n",
        "\n",
        "# Print out the shapes of our training and test datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Plot the training and test data\n",
        "plot_predictions(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Set the manual seed when creating the model (this isn't always needed but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.xpu.manual_seed(RANDOM_SEED)\n",
        "model_0 = LinearRegressionModel()\n",
        "#model_0, model_0.state_dict()\n",
        "\n",
        "# Check model device\n",
        "next(model_0.parameters()).device\n",
        "\n",
        "# Set model to GPU if it's available, otherwise it'll default to CPU\n",
        "model_0.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\n",
        "next(model_0.parameters()).device\n",
        "\n",
        "# Create loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), # optimize newly created model's parameters\n",
        "                            lr=learning_rate)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.xpu.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Set the number of epochs \n",
        "epochs = 1000 \n",
        "\n",
        "# Put data on the available device\n",
        "# Without this, error will happen (not all model/data on device)\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "if use_amp:\n",
        "    # We'll use GradScaler to help with mixed precision training\n",
        "    scaler = torch.amp.GradScaler(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_0.train() # train mode is on by default after construction\n",
        "\n",
        "    if use_amp:\n",
        "        # Forward pass with autocast for mixed precision\n",
        "        with torch.amp.autocast(device):\n",
        "            # 1. Forward pass\n",
        "            y_pred = model_0(X_train)\n",
        "\n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(y_pred, y_train)\n",
        "    else:\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X_train)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    # 3. Zero grad optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Step the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_0.eval() # put the model in evaluation mode for testing (inference)\n",
        "    # 1. Forward pass\n",
        "    with torch.inference_mode():\n",
        "        # Forward pass with autocast for mixed precision\n",
        "        \n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(device):\n",
        "                test_pred = model_0(X_test)\n",
        "        else:\n",
        "            test_pred = model_0(X_test)\n",
        "    \n",
        "        # 2. Calculate the loss\n",
        "        test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n",
        "\n",
        "# Find our model's learned parameters\n",
        "from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \n",
        "print(\"The model learned the following values for weights and bias:\")\n",
        "pprint(model_0.state_dict())\n",
        "print(\"\\nAnd the original values for weights and bias are:\")\n",
        "print(f\"weights: {weight}, bias: {bias}\")\n",
        "\n",
        "# Turn model into evaluation mode\n",
        "model_0.eval()\n",
        "\n",
        "# Make predictions on the test data\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_0(X_test)\n",
        "\n",
        "# Put data on the CPU and plot it\n",
        "plot_predictions(predictions=y_preds.cpu())\n",
        "\n",
        "# 1. Create models directory \n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path \n",
        "MODEL_NAME = \"LinearRegressionModel_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict \n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "           f=MODEL_SAVE_PATH) \n",
        "\n",
        "# Instantiate a fresh instance of LinearRegressionModelV2\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load model state dict \n",
        "loaded_model_0.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\n",
        "loaded_model_0.to(device)\n",
        "\n",
        "print(f\"Loaded model:\\n{loaded_model_0}\")\n",
        "print(f\"Model on device:\\n{next(loaded_model_0.parameters()).device}\")\n",
        "\n",
        "# Evaluate loaded model\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    loaded_model_0_preds = loaded_model_0(X_test)\n",
        "y_preds == loaded_model_0_preds"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
