{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quant the model to int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model and tokenizer saved to gemma-2-2b-it-int4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Optional: Use 'nf4' for Normal Float 4 quantization\n",
    ")\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # Automatically distribute the model across available devices\n",
    ")\n",
    "\n",
    "# Load the Italian tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/gemma-2b-it-tokenizer\")\n",
    "\n",
    "# Save the quantized model and tokenizer\n",
    "model_save_path = \"gemma-2-2b-it-int4\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Quantized model and tokenizer saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the quant int4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain as much as possible regarding pytorch and its applications.\n",
      "\n",
      "## PyTorch: A Deep Dive\n",
      "\n",
      "PyTorch is a powerful open-source machine learning framework developed by Facebook (now Meta). It's known for its flexibility, dynamic computation graph, and ease of use, making it a popular choice for researchers and developers alike.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Dynamic Computation Graph:** Unlike static graphs in frameworks like TensorFlow, PyTorch allows you to define and modify your model's structure during runtime. This flexibility is crucial for experimentation and debugging.\n",
      "* **Tensor Operations:** PyTorch leverages tensors, multi-dimensional arrays, for efficient numerical computations. Tensors are the building blocks of your data and models.\n",
      "* **GPU Acceleration:** PyTorch seamlessly integrates with GPUs, enabling faster training and inference for complex models.\n",
      "* **Pythonic API:** PyTorch's API is designed to be intuitive and familiar to Python users, making it easy to learn and use.\n",
      "* **Extensive Ecosystem:** PyTorch boasts a vibrant community and a rich ecosystem of libraries, tools, and pre-trained models.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "PyTorch's versatility makes it suitable for a wide range of applications, including:\n",
      "\n",
      "* **Computer Vision:** Image classification, object detection, image segmentation, and more.\n",
      "* **Natural Language Processing:** Text classification, sentiment analysis, machine translation, and language modeling.\n",
      "* **Reinforcement Learning:** Training agents to interact with environments and learn optimal strategies.\n",
      "* **Time Series Analysis:** Forecasting, anomaly detection, and pattern recognition in time-dependent data.\n",
      "* **Scientific Computing:** Solving complex scientific problems, such as drug discovery and climate modeling.\n",
      "* **Generative AI:** Creating new data, such as images, text, and music.\n",
      "\n",
      "**Advantages of PyTorch:**\n",
      "\n",
      "* **Flexibility:** Dynamic computation graph allows for easy model modification and experimentation.\n",
      "* **Ease of Use:** Pythonic API and intuitive syntax make it accessible to beginners.\n",
      "* **GPU Acceleration:** Enables faster training and inference for large models.\n",
      "* **Strong Community:** Active community provides support, resources, and pre-trained models.\n",
      "* **Research-Oriented:** Designed for research and development, with features like debugging tools and model visualization.\n",
      "\n",
      "**Getting Started with PyTorch:**\n",
      "\n",
      "1. **Installation:** Install PyTorch using pip: `pip install torch`\n",
      "2. **Basic Usage:** Import PyTorch and create a tensor: `import torch; x =\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the quantized model and tokenizer\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_new_tokens=500):\n",
    "    # Encode the prompt text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Please explain as much as possible regarding pytorch\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
